{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced class metrics\n",
    "- Class imbalance is something that can hamper your model's performance in any machine learning context. This is especially relevant in a machine learning interview if we are asked what to do if we are given a dataset with an imbalanced class, as some data is imbalanced by design such as insurance fraud data.\n",
    "- We'll use sklearn to create a logistic regression model and print the confusion matrix along with several evaluation metrics to get a better understanding of how to interpret Machine Learning models from datasets that have a class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and split\n",
    "\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The metrics aren't actually too bad. Precision of 0.72 means there might be a high number of false positives than you really want to see. Let's see if you can improve them in the next exercise with some resampling techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling techniques\n",
    "- We saw how class imbalance can impact the results of our confusion matrix. Here, we'll practice resampling techniques to explore the different results that alternative resampling styles can have on a dataset with class imbalance like that seen with loan_data. Using sklearn's resample() function, matching the number of rows in the majority class is called upsampling, while matching the number of rows in the minority class is called downsampling. \n",
    "- **Task** : create both an upsampled and downsampled version of the loan_data dataset, apply a logistic regression on both of them and then evaluate your performance. The training data and its labels that correspond to deny are subset to contain only the minority class and to approve that correspond to the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample minority and combine with majority\n",
    "loans_upsampled = resample(deny, replace=True, n_samples=len(approve), random_state=123)\n",
    "upsampled = pd.concat([approve, loans_upsampled])\n",
    "\n",
    "# Downsample majority and combine with minority\n",
    "loans_downsampled = resample(approve, replace = False,  n_samples = len(deny), random_state = 123)\n",
    "downsampled = pd.concat([loans_downsampled, deny])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an upsampled feature matrix and target array.\n",
    "- Instantiate a logistic regression model object, fit, and predict with X_test.\n",
    "- Print the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampled feature matrix and target array\n",
    "X_train_up = upsampled.drop('Loan Status', axis=1)\n",
    "y_train_up = upsampled['Loan Status']\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "loan_lr_up = LogisticRegression(solver='liblinear')\n",
    "loan_lr_up.fit(X_train_up, y_train_up)\n",
    "upsampled_y_pred = loan_lr_up.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, upsampled_y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, upsampled_y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, upsampled_y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, upsampled_y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, upsampled_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a downsampled feature matrix and target array.\n",
    "- Instantiate a logistic regression model object, fit, and predict with X_test.\n",
    "- Print the evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampled feature matrix and target array\n",
    "X_train_down = downsampled.drop('Loan Status', axis=1)\n",
    "y_train_down = downsampled['Loan Status']\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "loan_lr_down = LogisticRegression(solver='liblinear')\n",
    "loan_lr_down.fit(X_train_down, y_train_down)\n",
    "downsampled_y_pred = loan_lr_down.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, downsampled_y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, downsampled_y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, downsampled_y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, downsampled_y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, downsampled_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using both upsampling and downsampling techniques improved the precision score significantly, meaning there are less false positives. That is definitely a good thing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
